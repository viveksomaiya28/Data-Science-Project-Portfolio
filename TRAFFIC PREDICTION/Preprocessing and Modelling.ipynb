{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d75e530e-af21-44e8-8a67-8458ab46d035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             DateTime  day_of_month  week_of_year\n",
      "0 2015-11-01 00:00:00             1            44\n",
      "1 2015-11-01 01:00:00             1            44\n",
      "2 2015-11-01 02:00:00             1            44\n",
      "3 2015-11-01 03:00:00             1            44\n",
      "4 2015-11-01 04:00:00             1            44\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/traffic.csv')\n",
    "\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "\n",
    "#extra temporal features\n",
    "df['day_of_month'] = df['DateTime'].dt.day\n",
    "df['week_of_year'] = df['DateTime'].dt.isocalendar().week.astype(int)\n",
    "print(df[['DateTime', 'day_of_month', 'week_of_year']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d533f13e-e5d9-40e5-8608-79222b796510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48120, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa37f23a-8183-4031-89bf-79c4dcec60a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DateTime', 'Junction', 'Vehicles', 'ID', 'day_of_month',\n",
       "       'week_of_year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0810366c-719f-4786-82c2-57d195586cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Junction 1 missing timestamps count: 0\n",
      "Junction 2 missing timestamps count: 0\n",
      "Junction 3 missing timestamps count: 0\n",
      "Junction 4 missing timestamps count: 0\n",
      "\n",
      "Missing timestamps for Junction 4:\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq='h')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\AppData\\Local\\Temp\\ipykernel_17568\\4099908458.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_range = pd.date_range(start=df_j['DateTime'].min(), end=df_j['DateTime'].max(), freq='H')\n"
     ]
    }
   ],
   "source": [
    "missing_timestamps = {}\n",
    "\n",
    "for junction in df['Junction'].unique():\n",
    "    df_j = df[df['Junction'] == junction]\n",
    "    \n",
    "    # Create full hourly date range from min to max DateTime for this junction\n",
    "    full_range = pd.date_range(start=df_j['DateTime'].min(), end=df_j['DateTime'].max(), freq='H')\n",
    "    \n",
    "    # Find missing timestamps by set difference\n",
    "    missing = full_range.difference(df_j['DateTime'])\n",
    "    \n",
    "    missing_timestamps[junction] = missing\n",
    "    print(f'Junction {junction} missing timestamps count: {len(missing)}')\n",
    "\n",
    "#missing timestamps for junction 4:\n",
    "print(\"\\nMissing timestamps for Junction 4:\")\n",
    "print(missing_timestamps[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91d5160f-0cc5-43f8-bd0f-729df4df0ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             DateTime  Junction  Vehicles  Vehicles_lag_1  Vehicles_lag_2\n",
      "0 2015-11-01 00:00:00         1        15             NaN             NaN\n",
      "1 2015-11-01 01:00:00         1        13            15.0             NaN\n",
      "2 2015-11-01 02:00:00         1        10            13.0            15.0\n",
      "3 2015-11-01 03:00:00         1         7            10.0            13.0\n",
      "4 2015-11-01 04:00:00         1         9             7.0            10.0\n",
      "5 2015-11-01 05:00:00         1         6             9.0             7.0\n",
      "6 2015-11-01 06:00:00         1         9             6.0             9.0\n",
      "7 2015-11-01 07:00:00         1         8             9.0             6.0\n",
      "8 2015-11-01 08:00:00         1        11             8.0             9.0\n",
      "9 2015-11-01 09:00:00         1        12            11.0             8.0\n"
     ]
    }
   ],
   "source": [
    "#sort data by junction and datetime for correct lag calculation\n",
    "df = df.sort_values(['Junction', 'DateTime'])\n",
    "\n",
    "#creates lag features\n",
    "df['Vehicles_lag_1'] = df.groupby('Junction')['Vehicles'].shift(1)\n",
    "df['Vehicles_lag_2'] = df.groupby('Junction')['Vehicles'].shift(2)\n",
    "\n",
    "#first few rows to confirm lag features\n",
    "print(df[['DateTime', 'Junction', 'Vehicles', 'Vehicles_lag_1', 'Vehicles_lag_2']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59bfd8ee-32e2-48a3-ac3a-35f1441dde42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Vehicles_lag_1', 'Vehicles_lag_2']).copy()\n",
    "# more lag features (up to 6 hours ago)\n",
    "for lag in range(3, 7):\n",
    "    df[f'Vehicles_lag_{lag}'] = df.groupby('Junction')['Vehicles'].shift(lag)\n",
    "\n",
    "#rolling (moving) average features\n",
    "df['Vehicles_roll_mean_3'] = df.groupby('Junction')['Vehicles'].shift(1).rolling(window=3).mean()\n",
    "df['Vehicles_roll_mean_6'] = df.groupby('Junction')['Vehicles'].shift(1).rolling(window=6).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00f6aa9a-96cb-4c9a-8b0f-a5592af612d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              DateTime  Junction  Vehicles  Vehicles_lag_1  Vehicles_lag_2  \\\n",
      "2  2015-11-01 02:00:00         1        10            13.0            15.0   \n",
      "3  2015-11-01 03:00:00         1         7            10.0            13.0   \n",
      "4  2015-11-01 04:00:00         1         9             7.0            10.0   \n",
      "5  2015-11-01 05:00:00         1         6             9.0             7.0   \n",
      "6  2015-11-01 06:00:00         1         9             6.0             9.0   \n",
      "7  2015-11-01 07:00:00         1         8             9.0             6.0   \n",
      "8  2015-11-01 08:00:00         1        11             8.0             9.0   \n",
      "9  2015-11-01 09:00:00         1        12            11.0             8.0   \n",
      "10 2015-11-01 10:00:00         1        15            12.0            11.0   \n",
      "11 2015-11-01 11:00:00         1        17            15.0            12.0   \n",
      "\n",
      "    Vehicles_lag_3  Vehicles_lag_4  Vehicles_lag_5  Vehicles_lag_6  \\\n",
      "2              NaN             NaN             NaN             NaN   \n",
      "3              NaN             NaN             NaN             NaN   \n",
      "4              NaN             NaN             NaN             NaN   \n",
      "5             10.0             NaN             NaN             NaN   \n",
      "6              7.0            10.0             NaN             NaN   \n",
      "7              9.0             7.0            10.0             NaN   \n",
      "8              6.0             9.0             7.0            10.0   \n",
      "9              9.0             6.0             9.0             7.0   \n",
      "10             8.0             9.0             6.0             9.0   \n",
      "11            11.0             8.0             9.0             6.0   \n",
      "\n",
      "    Vehicles_roll_mean_3  Vehicles_roll_mean_6  \n",
      "2                    NaN                   NaN  \n",
      "3                    NaN                   NaN  \n",
      "4                    NaN                   NaN  \n",
      "5               8.666667                   NaN  \n",
      "6               7.333333                   NaN  \n",
      "7               8.000000                   NaN  \n",
      "8               7.666667              8.166667  \n",
      "9               9.333333              8.333333  \n",
      "10             10.333333              9.166667  \n",
      "11             12.666667             10.166667  \n",
      "Rows after dropping NaNs: 48112\n",
      "DateTime                 0\n",
      "Junction                 0\n",
      "Vehicles                 0\n",
      "ID                       0\n",
      "day_of_month             0\n",
      "week_of_year             0\n",
      "Vehicles_lag_1           0\n",
      "Vehicles_lag_2           0\n",
      "Vehicles_lag_3          12\n",
      "Vehicles_lag_4          16\n",
      "Vehicles_lag_5          20\n",
      "Vehicles_lag_6          24\n",
      "Vehicles_roll_mean_3    12\n",
      "Vehicles_roll_mean_6    24\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[['DateTime', 'Junction', 'Vehicles',\n",
    "          'Vehicles_lag_1', 'Vehicles_lag_2', 'Vehicles_lag_3', 'Vehicles_lag_4',\n",
    "          'Vehicles_lag_5', 'Vehicles_lag_6', 'Vehicles_roll_mean_3', 'Vehicles_roll_mean_6']].head(10))\n",
    "print(f\"Rows after dropping NaNs: {len(df)}\")\n",
    "print(df.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae439f0-e678-4b3c-8ab5-cf3be95dcf4e",
   "metadata": {},
   "source": [
    "## Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1af2415d-53e3-489a-bf67-2ccd093842f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df.dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d97419a2-8231-4362-a6d6-4337e290244d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\anaconda4\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0054 - val_loss: 0.0021\n",
      "Epoch 2/50\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 3/50\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 4/50\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 5/50\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 6/50\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 9.9108e-04 - val_loss: 0.0019\n",
      "Epoch 7/50\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 9.7361e-04 - val_loss: 0.0017\n",
      "Epoch 8/50\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 9.5643e-04 - val_loss: 0.0017\n",
      "Epoch 9/50\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 9.6219e-04 - val_loss: 0.0018\n",
      "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019\n",
      "Test Loss (MSE): 0.001333\n",
      "\u001b[1m301/301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "MAE: 3.5852\n",
      "MSE: 32.0360\n",
      "RMSE: 5.6600\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "feature_cols = [f'Vehicles_lag_{lag}' for lag in range(1, 7)]  # lag_1 to lag_6\n",
    "target_col = 'Vehicles'\n",
    "\n",
    "df_model = df_model.dropna(subset=feature_cols + [target_col])  # drops rows with NaNs in relevant cols\n",
    "\n",
    "X = df_model[feature_cols].values \n",
    "y = df_model[target_col].values    \n",
    "\n",
    "#Reshape X for LSTM input: (samples, timesteps, features)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))  # 6 timesteps, 1 feature\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "X_train_2d = X_train.reshape(-1, 1)\n",
    "X_test_2d = X_test.reshape(-1, 1)\n",
    "\n",
    "scaler_X.fit(X_train_2d)\n",
    "X_train_scaled = scaler_X.transform(X_train_2d).reshape(X_train.shape)\n",
    "X_test_scaled = scaler_X.transform(X_test_2d).reshape(X_test.shape)\n",
    "\n",
    "#Scales target values\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "scaler_y.fit(y_train)\n",
    "y_train_scaled = scaler_y.transform(y_train)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "#LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='tanh', input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "#model with early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#Evaluat on test set\n",
    "loss = model.evaluate(X_test_scaled, y_test_scaled)\n",
    "print(f'Test Loss (MSE): {loss:.6f}')\n",
    "\n",
    "# predictions and inverse scale\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_true = scaler_y.inverse_transform(y_test_scaled)\n",
    "\n",
    "#error metrics in original scale\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d89be-fd0e-4dc2-be26-daa89572029e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a65bc4-48cc-4141-b357-856fe19ddac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85181b32-0b02-41a1-96de-99a944a65473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
